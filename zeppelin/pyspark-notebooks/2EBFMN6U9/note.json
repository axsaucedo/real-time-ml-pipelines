{
  "paragraphs": [
    {
      "text": "%dep\n\nz.load(\"org.apache.spark:spark-sql_2.11:2.4.1\")",
      "user": "anonymous",
      "dateUpdated": "2019-04-23 17:31:27.170",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res0: org.apache.zeppelin.dep.Dependency \u003d org.apache.zeppelin.dep.Dependency@5a178b92\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556040657283_-1330756823",
      "id": "20190423-173057_1819333331",
      "dateCreated": "2019-04-23 17:30:57.284",
      "dateStarted": "2019-04-23 17:31:27.216",
      "dateFinished": "2019-04-23 17:32:23.024",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\ndf \u003d (sc.textFile(\"file:///machinelearning/reddit_200k_train.csv\")).toDF()",
      "user": "anonymous",
      "dateUpdated": "2019-04-23 17:45:38.671",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Fail to execute line 1: df \u003d (sc.textFile(\"file:///machinelearning/reddit_200k_train.csv\")).toDF()\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-3130422475988887227.py\", line 380, in \u003cmodule\u003e\n    exec(code, _zcUserQueryNameSpace)\n  File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e\n  File \"/opt/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/session.py\", line 57, in toDF\n    return sparkSession.createDataFrame(self, schema, sampleRatio)\n  File \"/opt/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/session.py\", line 535, in createDataFrame\n    rdd, schema \u003d self._createFromRDD(data.map(prepare), schema, samplingRatio)\n  File \"/opt/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/session.py\", line 375, in _createFromRDD\n    struct \u003d self._inferSchema(rdd, samplingRatio)\n  File \"/opt/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/session.py\", line 346, in _inferSchema\n    first \u003d rdd.first()\n  File \"/opt/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py\", line 1361, in first\n    rs \u003d self.take(1)\n  File \"/opt/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py\", line 1343, in take\n    res \u003d self.context.runJob(self, takeUpToNumLeft, p)\n  File \"/opt/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/context.py\", line 992, in runJob\n    port \u003d self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n  File \"/opt/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1131, in __call__\n    answer \u003d self.gateway_client.send_command(command)\n  File \"/opt/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 883, in send_command\n    response \u003d connection.send_command(command)\n  File \"/opt/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1028, in send_command\n    answer \u003d smart_decode(self.stream.readline()[:-1])\n  File \"/usr/lib/python3.6/socket.py\", line 586, in readinto\n    return self._sock.recv_into(b)\n  File \"/opt/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/context.py\", line 237, in signal_handler\n    raise KeyboardInterrupt()\nKeyboardInterrupt\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556036503016_2015038701",
      "id": "20190423-162143_2086171920",
      "dateCreated": "2019-04-23 16:21:43.016",
      "dateStarted": "2019-04-23 17:45:38.741",
      "dateFinished": "2019-04-23 17:48:06.353",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\ndf \u003d spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/machinelearning/reddit_200k_train.csv\")",
      "user": "anonymous",
      "dateUpdated": "2019-04-23 17:55:41.753",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[0;31m\u001b[0m\n\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)\n\u001b[0;32m\u003cipython-input-4-f586a80be371\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/machinelearning/reddit_200k_train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m \u001b[0m__zeppelin__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_displayhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m: \u0027IPySparkZeppelinContext\u0027 object has no attribute \u0027_displayhook\u0027"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://172.25.0.19:4040/jobs/job?id\u003d0"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1556038381328_-835348963",
      "id": "20190423-165301_2107108866",
      "dateCreated": "2019-04-23 16:53:01.328",
      "dateStarted": "2019-04-23 17:55:42.357",
      "dateFinished": "2019-04-23 17:55:56.802",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "user": "anonymous",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1556038386447_27862236",
      "id": "20190423-165306_1385855809",
      "dateCreated": "2019-04-23 16:53:06.447",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Model Development",
  "id": "2EBFMN6U9",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}