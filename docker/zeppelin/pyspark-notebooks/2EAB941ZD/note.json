{
  "paragraphs": [
    {
      "text": "%consumer.pyspark\n\nfrom pyspark.streaming import StreamingContext\n\n# SparkContext Config\nsc.setLogLevel(\"WARN\")\n\ntry:\n    # Reset streaming context if exists\n    ssc.stop(stopSparkContext\u003dFalse, stopGraceFully\u003dFalse)\nexcept:\n    pass\n\nssc \u003d StreamingContext(sc, batchDuration\u003d2)",
      "user": "anonymous",
      "dateUpdated": "2019-04-23 16:45:02.071",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1556023399287_172318548",
      "id": "20190423-124319_905844445",
      "dateCreated": "2019-04-23 12:43:19.287",
      "dateStarted": "2019-04-23 15:43:45.341",
      "dateFinished": "2019-04-23 15:43:45.403",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%consumer.pyspark\nfrom kafka import KafkaProducer\nfrom kafka.errors import KafkaError\nfrom pyspark.streaming.kafka import KafkaUtils\n\ntopic \u003d \"train\"\nbrokers \u003d \"172.25.0.12:9092,172.25.0.13:9092\"\n\n\nkvs \u003d KafkaUtils.createDirectStream(ssc, [topic], {\"metadata.broker.list\": brokers})\n\nlines \u003d kvs.map(lambda x: x[1])\n\nwords \u003d lines.flatMap(lambda line: line.split(\" \"))\n\nword_counts \u003d words.map(lambda word: (word, 1)) \\\n                 .reduceByKey(lambda a, b: a+b)\n\n# word_counts.pprint()\n\n\nfrom kafka import KafkaProducer\nfrom kafka.errors import KafkaError\n\ndef send_to_kafka(x):\n    producer \u003d KafkaProducer(bootstrap_servers\u003d[\"172.25.0.12:9092\"])\n    future \u003d producer.send(\"test\", str(x).encode(\"utf-8\"))\n    try:\n        record_metadata \u003d future.get(timeout\u003d10)\n    except KafkaError:\n        # Decide what to do if produce request failed...\n        log.exception()\n        pass\n\nword_counts.foreachRDD(lambda x: x.foreach(send_to_kafka))\n\nssc.start()\nssc.awaitTermination()",
      "user": "anonymous",
      "dateUpdated": "2019-04-23 15:43:50.599",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Fail to execute line 37: ssc.awaitTermination()\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-3517901777895046841.py\", line 380, in \u003cmodule\u003e\n    exec(code, _zcUserQueryNameSpace)\n  File \"\u003cstdin\u003e\", line 37, in \u003cmodule\u003e\n  File \"/opt/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/streaming/context.py\", line 206, in awaitTermination\n    self._jssc.awaitTermination()\n  File \"/opt/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1131, in __call__\n    answer \u003d self.gateway_client.send_command(command)\n  File \"/opt/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 883, in send_command\n    response \u003d connection.send_command(command)\n  File \"/opt/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1028, in send_command\n    answer \u003d smart_decode(self.stream.readline()[:-1])\n  File \"/usr/lib/python3.6/socket.py\", line 586, in readinto\n    return self._sock.recv_into(b)\n  File \"/opt/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/context.py\", line 237, in signal_handler\n    raise KeyboardInterrupt()\nKeyboardInterrupt\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556022009133_-1199138898",
      "id": "20190423-122009_873241770",
      "dateCreated": "2019-04-23 12:20:09.133",
      "dateStarted": "2019-04-23 15:43:50.633",
      "dateFinished": "2019-04-23 15:52:21.184",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%producer.pyspark\nimport time\nimport random\n\nfrom kafka import KafkaProducer\nfrom kafka.errors import KafkaError\n\n# SparkContext Config\nsc.setLogLevel(\"WARN\")\n\nwhile True:\n    producer \u003d KafkaProducer(bootstrap_servers\u003d[\"172.25.0.12:9092\"])\n    \n    chosen \u003d random.choice([\"hello\", \"my\", \"name\", \"is\", \"test\"])\n    future \u003d producer.send(\"train\", chosen.encode(\"utf-8\"))\n    try:\n        record_metadata \u003d future.get(timeout\u003d10)\n    except KafkaError:\n        # Decide what to do if produce request failed...\n        log.exception()\n        pass\n    \n    time.sleep(1)",
      "user": "anonymous",
      "dateUpdated": "2019-04-23 16:45:12.538",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Fail to execute line 20:     time.sleep(1)\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-4602704695007181253.py\", line 380, in \u003cmodule\u003e\n    exec(code, _zcUserQueryNameSpace)\n  File \"\u003cstdin\u003e\", line 20, in \u003cmodule\u003e\n  File \"/opt/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/context.py\", line 237, in signal_handler\n    raise KeyboardInterrupt()\nKeyboardInterrupt\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556030536741_-1566643662",
      "id": "20190423-144216_1146884124",
      "dateCreated": "2019-04-23 14:42:16.741",
      "dateStarted": "2019-04-23 15:44:26.674",
      "dateFinished": "2019-04-23 15:52:17.578",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%python\nimport os\nos.getcwd()",
      "user": "anonymous",
      "dateUpdated": "2019-04-23 15:53:32.851",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u0027/opt/zeppelin\u0027\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556032043210_365863747",
      "id": "20190423-150723_1199202103",
      "dateCreated": "2019-04-23 15:07:23.210",
      "dateStarted": "2019-04-23 15:53:32.895",
      "dateFinished": "2019-04-23 15:53:32.906",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%python\n",
      "user": "anonymous",
      "dateUpdated": "2019-04-23 15:53:32.852",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1556034812851_1853316686",
      "id": "20190423-155332_470186174",
      "dateCreated": "2019-04-23 15:53:32.851",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Comment Streams",
  "id": "2EAB941ZD",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "python:shared_process": [],
    "producer:shared_process": [],
    "spark:shared_process": [],
    "consumer:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {}
}